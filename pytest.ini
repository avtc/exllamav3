[tool:pytest]
# Test discovery and execution
minversion = 7.4
addopts = 
    # General options
    -v
    --tb=short
    --strict-markers
    --strict-config
    
    # Coverage options
    --cov=exllamav3
    --cov-report=term-missing
    --cov-report=html:htmlcov
    --cov-report=xml:coverage.xml
    --cov-fail-under=80
    
    # Performance and benchmarking
    --benchmark-only
    --benchmark-sort=mean
    --benchmark-group-by=param:backend
    --benchmark-warmup=on
    --benchmark-warmup-iterations=3
    --benchmark-timeout=300
    
    # Parallel testing
    --numprocesses=auto
    --dist=worksteal
    --maxfail=10
    
    # Output formatting
    --durations=20
    --disable-warnings

# Test markers
markers =
    unit: Unit tests for individual components
    integration: Integration tests for multiple components
    performance: Performance benchmarking tests
    stress: Stress tests for performance validation
    slow: Tests that take a long time to run
    gpu: Tests that require GPU (will be skipped if no GPU)
    p2p: P2P-specific tests
    nccl: NCCL backend tests
    native: Native backend tests
    auto: Auto-selection logic tests
    error: Error handling and recovery tests
    mock: Tests that use mocking
    serial: Tests that must run serially
    flaky: Tests that are known to be flaky

# Test paths
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*

# Filter warnings
filterwarnings =
    ignore::DeprecationWarning
    ignore::PendingDeprecationWarning
    ignore::UserWarning:torch.*
    ignore::UserWarning:torch.cuda.*
    ignore::RuntimeWarning:torch.*
    ignore::ImportWarning:torch.*
    ignore::ImportWarning:exllamav3.*

# Custom options for P2P testing
markers_p2p = 
    unit,p2p: Unit tests for P2P backend
    integration,p2p: Integration tests for P2P backend
    performance,p2p: Performance tests for P2P backend
    error,p2p: Error handling tests for P2P backend
    auto,p2p: Auto-selection tests for P2P backend

# Benchmark configuration
[benchmark]
# Benchmark settings
min_rounds = 5
max_time = 30.0
min_time = 0.1
warmup = 3
sort = mean
group_by = param:backend